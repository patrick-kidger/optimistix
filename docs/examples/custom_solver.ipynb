{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67622eab-2d7d-4488-b698-1b93bcb8bbb5",
   "metadata": {},
   "source": [
    "# Hybridising solvers\n",
    "\n",
    "The ability to create custom solvers is one of the most powerful things about Optimistix. This is a great playground for the advanced user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdfb27-77aa-4293-8a3c-cf28ee19d001",
   "metadata": {},
   "source": [
    "## Approach 1: mix-and-match using existing APIs.\n",
    "\n",
    "Many abstract solvers have `search` and `descent` fields. The first is a choice of line search, trust region, or learning rate. Optimistix uses a generalised notion of all three. The second is a choice of what it means to \"move downhill\": for example this could be [`optimistix.SteepestDescent`][] to use a local linear approximation, or [`optimistix.NewtonDescent`][] to use a local quadratic approximation.\n",
    "\n",
    "See the [searches and descents](../api/searches/introduction.md) page for more details on this idea.\n",
    "\n",
    "Here's quick demo of how to create a novel minimiser using this. This example uses a BFGS quasi-Newton approximation to the Hessian of a minimisation problem. This approximation is used to build a piecwise-linear dogleg-shaped descent path (interpolating between steepest descsent and Newton desscent). How far we move along this path is then determined by a trust region algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a701a676-324e-4cb0-95a6-882242ae6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "import optimistix as optx\n",
    "\n",
    "\n",
    "class MyNewMinimiser(optx.AbstractBFGS):\n",
    "    rtol: float\n",
    "    atol: float\n",
    "    norm: Callable = optx.max_norm\n",
    "    use_inverse: bool = False\n",
    "    descent: optx.AbstractDescent = optx.DoglegDescent()\n",
    "    search: optx.AbstractSearch = optx.ClassicalTrustRegion\n",
    "\n",
    "\n",
    "solver = MyNewMinimiser(rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2ec99-6da1-4dca-b4b0-f5d053de728e",
   "metadata": {},
   "source": [
    "## Approach 2: create whole-new solvers, searches, and descents.\n",
    "\n",
    "This can be done by subclassing the relevant object; see the page on [abstract base classes](../abstract.md). For example, here's how we might describe a Newton descent direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8efe79-72ce-4c06-be11-44206ccb912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax.tree_util as jtu\n",
    "import lineax as lx  # https://github.com/google/lineax\n",
    "from jaxtyping import Array, PyTree  # https://github.com/google/jaxtyping\n",
    "\n",
    "\n",
    "class NewtonDescentState(eqx.Module):\n",
    "    newton: PyTree[Array]\n",
    "    result: optx.RESULTS\n",
    "\n",
    "\n",
    "class NewtonDescent(optx.AbstractDescent):\n",
    "    def init(self, y, f_info_struct):\n",
    "        del f_info_struct\n",
    "        # Dummy values of the right shape; unused.\n",
    "        return NewtonDescentState(y, optx.RESULTS.successful)\n",
    "\n",
    "    def query(self, y, f_info, state):\n",
    "        del state\n",
    "        if isinstance(f_info, optx.FunctionInfo.EvalGradHessianInv):\n",
    "            newton = f_info.hessian_inv.mv(f_info.grad)\n",
    "            result = optx.RESULTS.successful\n",
    "        else:\n",
    "            if isinstance(f_info, optx.FunctionInfo.EvalGradHessian):\n",
    "                operator = f_info.hessian\n",
    "                vector = f_info.grad\n",
    "            elif isinstance(f_info, optx.FunctionInfo.ResidualJac):\n",
    "                operator = f_info.jac\n",
    "                vector = f_info.residual\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Cannot use a Newton descent with a solver that only evaluates the \"\n",
    "                    \"gradient, or only the function itself.\"\n",
    "                )\n",
    "            out = lx.linear_solve(operator, vector)\n",
    "            newton = out.value\n",
    "            result = optx.RESULTS.promote(out.result)\n",
    "        return NewtonDescentState(newton, result)\n",
    "\n",
    "    def step(self, step_size, state):\n",
    "        return jtu.tree_map(lambda x: -step_size * x, state.newton), state.result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
